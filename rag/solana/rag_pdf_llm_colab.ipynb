{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RAG System with Hugging Face Models\n",
    "# For Google Colab with Hugging Face login\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers accelerate sentence-transformers PyMuPDF huggingface_hub -q\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from google.colab import files\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "# Login to Hugging Face\n",
    "def hf_login():\n",
    "    print(\"Please enter your Hugging Face token to access gated models.\")\n",
    "    print(\"You can find your token at https://huggingface.co/settings/tokens\")\n",
    "    token = getpass.getpass(\"Hugging Face Token: \")\n",
    "    login(token=token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Upload the PDF file\n",
    "def upload_pdf():\n",
    "    print(\"Please upload your PDF file...\")\n",
    "    uploaded = files.upload()\n",
    "    file_path = list(uploaded.keys())[0]\n",
    "    return file_path\n",
    "\n",
    "# Load the dataset from PDF\n",
    "def load_pdf_dataset(pdf_path):\n",
    "    dataset = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Different strategies for breaking down PDF content\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        # Split long pages into smaller chunks\n",
    "        chunk_size = 1000  # characters per chunk\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        dataset.extend(chunks)\n",
    "    doc.close()\n",
    "    return dataset\n",
    "\n",
    "# Vector Database\n",
    "VECTOR_DB = []\n",
    "\n",
    "# Load models from Hugging Face - optimized for Colab\n",
    "def load_models():\n",
    "    print(\"Loading embedding model...\")\n",
    "    # Load embedding model (sentence-transformers model for embeddings)\n",
    "    embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "    \n",
    "    print(\"Loading language model...\")\n",
    "    # Choose a smaller model that will fit in Colab's memory\n",
    "    # You can try different models based on the GPU you have available\n",
    "    llm_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or try \"google/flan-t5-base\" for a smaller model\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    if llm_tokenizer.pad_token is None:\n",
    "        llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    \n",
    "    # Load the model with optimizations for Colab\n",
    "    # Check available GPU memory and adjust accordingly\n",
    "    llm_model = None\n",
    "    try:\n",
    "        # Try loading with 8-bit quantization first\n",
    "        print(\"Attempting to load model with 8-bit quantization...\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"8-bit loading failed: {e}\")\n",
    "        try:\n",
    "            # Try with 4-bit quantization if 8-bit fails\n",
    "            print(\"Attempting to load model with 4-bit quantization...\")\n",
    "            llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                llm_model_name,\n",
    "                device_map=\"auto\",\n",
    "                load_in_4bit=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"4-bit loading failed: {e}\")\n",
    "            try:\n",
    "                # Try without quantization but with half precision\n",
    "                print(\"Attempting to load model with half precision...\")\n",
    "                llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    llm_model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Half precision loading failed: {e}\")\n",
    "                # Fall back to a much smaller model if all else fails\n",
    "                print(\"Falling back to a smaller model...\")\n",
    "                llm_model_name = \"google/flan-t5-base\"\n",
    "                llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "                llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    llm_model_name,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "    \n",
    "    print(f\"Successfully loaded models. LLM: {llm_model_name}\")\n",
    "    return {\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"embedding_tokenizer\": embedding_tokenizer,\n",
    "        \"llm_model\": llm_model,\n",
    "        \"llm_tokenizer\": llm_tokenizer\n",
    "    }\n",
    "\n",
    "# Function to try different models if one fails\n",
    "def try_alternative_models():\n",
    "    # List of models to try in order of preference\n",
    "    model_options = [\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        \"facebook/opt-2.7b\",\n",
    "        \"google/flan-t5-xl\",\n",
    "        \"google/flan-t5-base\",\n",
    "        \"gpt2-medium\"  # Very small fallback\n",
    "    ]\n",
    "    \n",
    "    for model_name in model_options:\n",
    "        print(f\"Trying to load {model_name}...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                \n",
    "            # Try different loading configurations\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    load_in_8bit=True,\n",
    "                    torch_dtype=torch.float16\n",
    "                )\n",
    "            except:\n",
    "                try:\n",
    "                    model = AutoModelForCausalLM.from_pretrained(\n",
    "                        model_name,\n",
    "                        device_map=\"auto\",\n",
    "                        load_in_4bit=True,\n",
    "                        torch_dtype=torch.float16\n",
    "                    )\n",
    "                except:\n",
    "                    model = AutoModelForCausalLM.from_pretrained(\n",
    "                        model_name,\n",
    "                        device_map=\"auto\",\n",
    "                        torch_dtype=torch.float16\n",
    "                    )\n",
    "            \n",
    "            print(f\"Successfully loaded {model_name}\")\n",
    "            return model, tokenizer, model_name\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "    \n",
    "    raise Exception(\"Could not load any language model. Please check your connection and try again.\")\n",
    "\n",
    "# Generate embeddings using Hugging Face model\n",
    "def generate_embedding(text, models):\n",
    "    # Tokenize and get embedding\n",
    "    tokenizer = models[\"embedding_tokenizer\"]\n",
    "    model = models[\"embedding_model\"]\n",
    "    \n",
    "    # Add padding and truncation\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use the mean of the last hidden state as the sentence embedding\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "def add_chunk_to_database(chunk, models):\n",
    "    try:\n",
    "        # Generate embedding for each chunk\n",
    "        embedding = generate_embedding(chunk, models)\n",
    "        VECTOR_DB.append((chunk, embedding))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding chunk: {e}\")\n",
    "        return False\n",
    "\n",
    "def calculate_similarity(query_embedding, doc_embedding):\n",
    "    query_embedding = torch.tensor(query_embedding)\n",
    "    doc_embedding = torch.tensor(doc_embedding)\n",
    "    return cosine_similarity(query_embedding.unsqueeze(0), doc_embedding.unsqueeze(0)).item()\n",
    "\n",
    "def retrieve(query, models, top_n=3):\n",
    "    try:\n",
    "        # Get query embedding\n",
    "        query_embedding = generate_embedding(query, models)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [\n",
    "            (chunk, calculate_similarity(query_embedding, embedding))\n",
    "            for chunk, embedding in VECTOR_DB\n",
    "        ]\n",
    "        \n",
    "        # Sort by similarity, descending\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "    except Exception as e:\n",
    "        print(f\"Retrieval error: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_response(query, context, models):\n",
    "    try:\n",
    "        tokenizer = models[\"llm_tokenizer\"]\n",
    "        model = models[\"llm_model\"]\n",
    "        \n",
    "        # Prepare prompt with context and question\n",
    "        prompt = f\"\"\"You are an expert in smart contract security.\n",
    "Use only the following pieces of context to answer the question.\n",
    "Do not make up any new information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode the response, removing the input prompt\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        generated_text = tokenizer.decode(generated_ids[0][input_length:], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"I encountered an error while generating a response.\"\n",
    "\n",
    "def main():\n",
    "    print(\"Checking for GPU availability...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Login to Hugging Face\n",
    "    hf_login()\n",
    "    \n",
    "    # Try loading models - first try standard approach, then fallback to alternatives if needed\n",
    "    try:\n",
    "        print(\"Loading models from Hugging Face...\")\n",
    "        models = load_models()\n",
    "        print(\"Models loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models with standard approach: {e}\")\n",
    "        print(\"Trying alternative models...\")\n",
    "        llm_model, llm_tokenizer, llm_model_name = try_alternative_models()\n",
    "        \n",
    "        # Load embedding model separately\n",
    "        embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "        embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        \n",
    "        models = {\n",
    "            \"embedding_model\": embedding_model,\n",
    "            \"embedding_tokenizer\": embedding_tokenizer,\n",
    "            \"llm_model\": llm_model,\n",
    "            \"llm_tokenizer\": llm_tokenizer\n",
    "        }\n",
    "        print(f\"Successfully loaded alternative models. Using {llm_model_name}\")\n",
    "    \n",
    "    # Upload and load the PDF file\n",
    "    pdf_path = upload_pdf()\n",
    "    \n",
    "    # Load dataset from PDF\n",
    "    print(\"Loading dataset from PDF...\")\n",
    "    dataset = load_pdf_dataset(pdf_path)\n",
    "    print(f'Loaded {len(dataset)} chunks')\n",
    "    \n",
    "    # Populate vector database\n",
    "    print(\"Building vector database...\")\n",
    "    successful_chunks = 0\n",
    "    for i, chunk in tqdm(enumerate(dataset), total=len(dataset), desc=\"Processing chunks\"):\n",
    "        if add_chunk_to_database(chunk, models):\n",
    "            successful_chunks += 1\n",
    "    \n",
    "    print(f\"Vector database built successfully with {successful_chunks} chunks!\")\n",
    "    \n",
    "    # Interactive Retrieval and Chatbot\n",
    "    print(\"\\n=== Smart Contract Security Assistant ===\")\n",
    "    while True:\n",
    "        input_query = input('\\nAsk a question about smart contract vulnerabilities (or type \"exit\"): ')\n",
    "        if input_query.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        print(\"Retrieving relevant information...\")\n",
    "        retrieved_knowledge = retrieve(input_query, models)\n",
    "        if not retrieved_knowledge:\n",
    "            print(\"No relevant information found.\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare context for language model\n",
    "        context = '\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])\n",
    "        \n",
    "        # Generate and print response\n",
    "        print('Generating response...')\n",
    "        response = generate_response(input_query, context, models)\n",
    "        print(f\"\\nResponse:\\n{response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
